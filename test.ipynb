{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('disser': conda)"
  },
  "interpreter": {
   "hash": "de4a326d1af7be3981cb728d2c4eed6e24675e020b4ba825e6ef7b2ba986d345"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Basic data test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy.sparse as sp"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A:\n tensor([[1, 1, 1, 0, 1],\n        [0, 0, 1, 1, 0],\n        [0, 1, 0, 0, 1],\n        [0, 0, 0, 1, 1],\n        [1, 0, 0, 1, 0]])\nA2:\n tensor([[1, 0, 1, 1, 0],\n        [0, 1, 1, 1, 0],\n        [0, 1, 0, 0, 1],\n        [0, 1, 1, 1, 0],\n        [1, 1, 1, 0, 0]])\nX:\n tensor([[0.0056, 0.5696, 0.6929],\n        [0.8857, 0.8973, 0.4834],\n        [0.7216, 0.0573, 0.9799],\n        [0.3912, 0.8353, 0.5471],\n        [0.3699, 0.6445, 0.2317]])\n"
     ]
    }
   ],
   "source": [
    "A=torch.randint(2,size=(5,5))\n",
    "A2=torch.randint(2,size=(5,5))\n",
    "X=torch.rand(5,3)\n",
    "print('A:\\n',A)\n",
    "print('A2:\\n',A2)\n",
    "print('X:\\n',X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[2, 1, 2, 1, 1],\n        [0, 1, 2, 2, 0],\n        [0, 2, 0, 0, 2],\n        [0, 1, 1, 2, 1],\n        [2, 1, 1, 1, 0]])\n"
     ]
    }
   ],
   "source": [
    "#Test window\n",
    "print(A+A2)"
   ]
  },
  {
   "source": [
    "# GATE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (create_logger, set_random_seed, rm_suffix,\n",
    "                   mkdir_if_no_exists)\n",
    "from utils import (read_meta, read_probs, l2norm, knns2ordered_nbrs,\n",
    "                   intdict2ndarray, Timer)\n",
    "from utils import (write_meta, write_feat)\n",
    "from utils.knn import *\n",
    "\n",
    "from mmcv import Config \n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n"
   ]
  },
  {
   "source": [
    "## load data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config='/dcs/pg20/u2085214/fc/learn-to-cluster/test_train_cfg_trial.py'\n",
    "cfg = Config.fromfile(config)\n",
    "\n",
    "cfg.phase = 'train'\n",
    "cfg.cuda = torch.cuda.is_available()\n",
    "cfg.load_from = None\n",
    "cfg.resume_from = None\n",
    "cfg.gpus = 1\n",
    "cfg.distributed = False\n",
    "cfg.save_output = False\n",
    "cfg.no_cuda = False\n",
    "cfg.force = False\n",
    "cfg.work_dir = './data/'\n",
    "cfg.cut_name = '_cut'\n",
    "\n",
    "for k, v in cfg.model['kwargs'].items(): #kwargs=dict(feature_dim=256)\n",
    "    setattr(cfg.train_data, k, v) #k? v?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from utils import (read_meta, read_probs, l2norm, knns2ordered_nbrs,\n",
    "                   intdict2ndarray, Timer)\n",
    "\n",
    "\n",
    "class ClusterDataset(object):\n",
    "    def __init__(self, cfg):\n",
    "        feat_path = cfg['feat_path']\n",
    "        label_path = cfg.get('label_path', None)\n",
    "        knn_graph_path = cfg['knn_graph_path']\n",
    "\n",
    "        self.k_at_hop = cfg['k_at_hop'] #k_at_hop=[100, 10], #200,10\n",
    "        self.depth = len(self.k_at_hop) #e.g. 2 : legth for k_at_hop\n",
    "        self.active_connection = cfg['active_connection']\n",
    "        self.feature_dim = cfg['feature_dim']\n",
    "        self.is_norm_feat = cfg.get('is_norm_feat', True) #normalized\n",
    "        self.is_sort_knns = cfg.get('is_sort_knns', True) #sorted knn\n",
    "        self.is_test = cfg.get('is_test', False) #depends on the train or test\n",
    "        \n",
    "        with Timer('read meta and feature'):\n",
    "            if label_path is not None:\n",
    "                _, idx2lb = read_meta(label_path) #e.g. {0:0,1:0,2:0 ...} dict format\n",
    "                self.inst_num = len(idx2lb) #instance num = # of data \n",
    "                self.labels = intdict2ndarray(idx2lb) #no. of class=8573 [   0.    0.    0. ... 8572. 8572. 8572.]\n",
    "                self.ignore_label = False\n",
    "            else:\n",
    "                self.labels = None\n",
    "                self.inst_num = -1\n",
    "                self.ignore_label = True\n",
    "            self.features = read_probs(feat_path, self.inst_num,\n",
    "                                       self.feature_dim)#self.feature.shape:(576494, 256)\n",
    "     \n",
    "            if self.is_norm_feat:\n",
    "                self.features = l2norm(self.features)\n",
    "            if self.inst_num == -1:\n",
    "                self.inst_num = self.features.shape[0]\n",
    "            self.size = self.inst_num\n",
    "\n",
    "        with Timer('read knn graph'):\n",
    "            knns = np.load(knn_graph_path)['data']\n",
    "            _, self.knn_graph = knns2ordered_nbrs(knns, sort=self.is_sort_knns)\n",
    "        assert np.mean(self.k_at_hop) >= self.active_connection\n",
    "\n",
    "        print('feature shape: {}, norm_feat: {}, sort_knns: {} '\n",
    "              'k_at_hop: {}, active_connection: {}'.format(\n",
    "                  self.features.shape, self.is_norm_feat, self.is_sort_knns,\n",
    "                  self.k_at_hop, self.active_connection))\n",
    "        print('labels shape:', self.labels.shape) #(584013,)\n",
    "        print('knns_graph shape:', self.knn_graph.shape) #(584013, 80)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        return the vertex feature and the adjacent matrix A, together\n",
    "        with the indices of the center node and its 1-hop nodes\n",
    "        '''\n",
    "        if index is None or index > self.size:\n",
    "            raise ValueError('index({}) is not in the range of {}'.format(\n",
    "                index, self.size))\n",
    "\n",
    "        center_node = index #428572 each index\n",
    "        \n",
    "        # hops[0] for 1-hop neighbors, hops[1] for 2-hop neighbors\n",
    "        hops = []\n",
    "        hops.append(set(self.knn_graph[center_node][1:]))\n",
    "        \n",
    "        # Actually we dont need the loop since the depth is fixed here,\n",
    "        # But we still remain the code for further revision\n",
    "        for d in range(1, self.depth):\n",
    "            hops.append(set())\n",
    "            for h in hops[-2]:\n",
    "                hops[-1].update(set(self.knn_graph[h][1:self.k_at_hop[d] + 1]))\n",
    "\n",
    "        hops_set = set([h for hop in hops for h in hop])\n",
    "        hops_set.update([\n",
    "            center_node,\n",
    "        ])\n",
    "\n",
    "        uniq_nodes = np.array(list(hops_set), dtype=np.int64)\n",
    "        uniq_nodes_map = {j: i for i, j in enumerate(uniq_nodes)}\n",
    "\n",
    "        center_idx = np.array([uniq_nodes_map[center_node]], dtype=np.int64)\n",
    "        one_hop_idxs = np.array([uniq_nodes_map[i] for i in hops[0]],\n",
    "                                dtype=np.int64)\n",
    "        center_feat = self.features[center_node]\n",
    "        feat = self.features[uniq_nodes]\n",
    "        feat = feat - center_feat\n",
    "\n",
    "        max_num_nodes = self.k_at_hop[0] * (self.k_at_hop[1] + 1) + 1\n",
    "        num_nodes = len(uniq_nodes)\n",
    "\n",
    "\n",
    "        # print('hops size[0]',len(hops[0]))\n",
    "        # print('hops size[1]',len(hops[1]))\n",
    "        # print('hops_set size',len(hops_set))\n",
    "        # print('index[{}] num_node: max={} uniq={}'.format(index,max_num_nodes,num_nodes))\n",
    "        # print('one_hop_idxs shape:',one_hop_idxs.shape)\n",
    "        # print('ceter_node:',center_node)\n",
    "        # print('feat shape',feat.shape)\n",
    "\n",
    "        A = np.zeros([num_nodes, num_nodes], dtype=feat.dtype)\n",
    "\n",
    "        res_num_nodes = max_num_nodes - num_nodes\n",
    "        if res_num_nodes > 0:\n",
    "            pad_feat = np.zeros([res_num_nodes, self.feature_dim],\n",
    "                                dtype=feat.dtype)\n",
    "            feat = np.concatenate([feat, pad_feat], axis=0)\n",
    "      \n",
    "        for node in uniq_nodes:\n",
    "            neighbors = self.knn_graph[node, 1:self.active_connection + 1]\n",
    "            for n in neighbors:\n",
    "                if n in uniq_nodes:\n",
    "                    i, j = uniq_nodes_map[node], uniq_nodes_map[n]\n",
    "                    A[i, j] = 1\n",
    "                    A[j, i] = 1\n",
    "\n",
    "        D = A.sum(1, keepdims=True)\n",
    "        A = A / D\n",
    "        A_ = np.zeros([max_num_nodes, max_num_nodes], dtype=A.dtype)\n",
    "        A_[:num_nodes, :num_nodes] = A\n",
    "\n",
    "        if self.ignore_label:\n",
    "            return (feat, A_, center_idx, one_hop_idxs)\n",
    "\n",
    "        labels = self.labels[uniq_nodes]\n",
    "        one_hop_labels = labels[one_hop_idxs]\n",
    "        center_label = labels[center_idx]\n",
    "        edge_labels = (center_label == one_hop_labels).astype(np.int64)\n",
    "\n",
    "        # print('feat shape={}, A_ shape={}, one_hop_idxs shape={},'.format(feat.shape))\n",
    "\n",
    "\n",
    "        if self.is_test:\n",
    "            if res_num_nodes > 0:\n",
    "                pad_nodes = np.zeros(res_num_nodes, dtype=uniq_nodes.dtype)\n",
    "                uniq_nodes = np.concatenate([uniq_nodes, pad_nodes], axis=0)\n",
    "            return (feat, A_, one_hop_idxs,\n",
    "                    edge_labels), center_idx, uniq_nodes\n",
    "        else:\n",
    "            return (feat, A_, one_hop_idxs, edge_labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmcv.runner import get_dist_info\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dsgcn.datasets.sampler import (DistributedSampler,\n",
    "                                    DistributedSequentialSampler)\n",
    "\n",
    "\n",
    "def build_dataloader(dataset,\n",
    "                     batch_size_per_gpu,\n",
    "                     workers_per_gpu,\n",
    "                     shuffle=False,\n",
    "                     train=False,\n",
    "                     **kwargs):\n",
    "    rank, world_size = get_dist_info()\n",
    "    if train:\n",
    "        sampler = DistributedSampler(dataset, world_size, rank, shuffle)\n",
    "    else:\n",
    "        sampler = DistributedSequentialSampler(dataset, world_size, rank)\n",
    "    batch_size = batch_size_per_gpu\n",
    "    num_workers = workers_per_gpu\n",
    "\n",
    "    data_loader = DataLoader(dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             sampler=sampler,\n",
    "                             num_workers=num_workers,\n",
    "                             pin_memory=False,\n",
    "                             **kwargs)\n",
    "\n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[./data/labels/part0_train_cut.meta] #cls: 45, #inst: 2948\n[Time] read meta and feature consumes 0.0101 s\n[Time] read knn graph consumes 0.0319 s\nfeature shape: (2948, 256), norm_feat: True, sort_knns: True k_at_hop: [50, 10], active_connection: 10\nlabels shape: (2948,)\nknns_graph shape: (2948, 80)\n"
     ]
    }
   ],
   "source": [
    "dataset = ClusterDataset(cfg.train_data)\n",
    "train_dataloader = build_dataloader(dataset,\n",
    "                         cfg.batch_size_per_gpu,\n",
    "                         cfg.workers_per_gpu,\n",
    "                         train=True,\n",
    "                         shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "one_hop_idxs shape=torch.Size([16, 79])\n one_hope_idxs e.g.=tensor([[  0,   1,   2,  ...,  91,  92,  94],\n        [  0,   1,   2,  ...,  87,  88,  90],\n        [  0,   1,   3,  ...,  88,  89,  90],\n        ...,\n        [  0,   1,   2,  ...,  82,  83,  84],\n        [ 14,  25,  28,  ..., 224, 225, 242],\n        [  0,   1,   2,  ...,  79,  80,  81]])\n\n\nA shape=torch.Size([16, 551, 551])\nA e.g.=tensor([[0.0000, 0.0476, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n        [0.0833, 0.0000, 0.0833,  ..., 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0455, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n        ...,\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])\n\n\nX shape=torch.Size([16, 551, 256])\nx e.g.=tensor([[ 0.0080,  0.0042, -0.0511,  ...,  0.0111,  0.0433,  0.0533],\n        [ 0.0620,  0.0388, -0.0499,  ...,  0.0383,  0.0207,  0.1123],\n        [ 0.0088,  0.0206, -0.0360,  ...,  0.0114,  0.0359,  0.0466],\n        ...,\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n\n\nlabels shape=torch.Size([16, 79])\nlabels e.g.=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1])\n\n\nlabels.view(-1) shape=torch.Size([1264])\nlabels.view(-1) e.g.=1\n\n\n"
     ]
    }
   ],
   "source": [
    "#print train_data.\n",
    "X, A, one_hop_idxs, labels = next(iter(train_dataloader))\n",
    "\n",
    "print('one_hop_idxs shape={}\\n one_hope_idxs e.g.={}\\n\\n'.format(one_hop_idxs.shape,one_hop_idxs))\n",
    "print('A shape={}\\nA e.g.={}\\n\\n'.format(A.shape,A[0,:,:]))\n",
    "print('X shape={}\\nx e.g.={}\\n\\n'.format(X.shape,X[0,:,:]))\n",
    "# print('pred shape={}\\npred e.g.={}'.format(pred.shape,pred[0]))\n",
    "print('labels shape={}\\nlabels e.g.={}\\n\\n'.format(labels.shape,labels[0]))\n",
    "print('labels.view(-1) shape={}\\nlabels.view(-1) e.g.={}\\n\\n'.format(labels.view(-1).shape,labels.view(-1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[0.1578, 0.1072, 0.9423],\n         [0.6200, 0.1926, 0.9997],\n         [0.4782, 0.0496, 0.9224],\n         [0.0331, 0.0592, 0.8493],\n         [0.4547, 0.5800, 0.2596]],\n\n        [[0.8982, 0.5207, 0.9766],\n         [0.9080, 0.0033, 0.9275],\n         [0.5973, 0.9254, 0.0974],\n         [0.4384, 0.1433, 0.4124],\n         [0.7530, 0.1531, 0.6214]]])\ntensor([[[0, 0, 0, 0, 1],\n         [0, 0, 0, 1, 1],\n         [0, 1, 1, 0, 1],\n         [1, 0, 1, 1, 0],\n         [0, 1, 0, 1, 0]],\n\n        [[1, 0, 0, 0, 0],\n         [1, 1, 0, 1, 1],\n         [0, 1, 1, 0, 1],\n         [1, 0, 0, 0, 1],\n         [0, 1, 1, 1, 1]]])\n"
     ]
    }
   ],
   "source": [
    "#simple matrix calculation and dimension check\n",
    "A=torch.randint(2,size=(5,5))\n",
    "A2=torch.randint(2,size=(5,5))\n",
    "X=torch.rand(5,3)\n",
    "\n",
    "AA=torch.randint(2,size=(2,5,5))\n",
    "AA2=torch.randint(2,size=(2,5,5))\n",
    "XX=torch.rand(2,5,3)\n",
    "XX2=torch.randint(3,size=(2,5,3))\n",
    "\n",
    "print(XX)\n",
    "print(AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mat1=torch.Size([2, 5, 5]) mat2=torch.Size([2, 5, 3])\nresult shape: torch.Size([2, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "mat1=AA\n",
    "mat2=XX2\n",
    "print('mat1={} mat2={}'.format(mat1.shape, mat2.shape))\n",
    "res=torch.matmul(mat1,mat2)\n",
    "print('result shape:',res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([256, 128])\ntorch.Size([1, 128])\ntorch.Size([1, 128])\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x128 and 5x3)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-72a4cdcce34d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph_Attention_Layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-72a4cdcce34d>\u001b[0m in \u001b[0;36mGraph_Attention_Layer\u001b[0;34m(self, A, H)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mGraph_Attention_Layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mMs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# vs*H cf H=H(k)=W(k)H(k-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mMs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# A*vs*H\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mMr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# vr*H\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x128 and 5x3)"
     ]
    }
   ],
   "source": [
    "#test print shape\n",
    "class test():\n",
    "    def __init__(self, input_dim, output_dim, **kwargs):\n",
    "        self.W, self.vr, self.vs = self.define_weight(input_dim, output_dim)\n",
    "\n",
    "    def define_weight(self, input_dim, output_dim):\n",
    "        init_range = np.sqrt(6.0/(input_dim + output_dim))\n",
    "        W_init = torch.rand(input_dim, output_dim)*2*init_range - init_range\n",
    "        W = nn.Parameter(W_init)\n",
    "        v_init = torch.rand(1, output_dim)*2*init_range - init_range\n",
    "        vr = nn.Parameter(v_init)\n",
    "        vs = nn.Parameter(v_init)\n",
    "        return W, vr, vs\n",
    "\n",
    "    def Graph_Attention_Layer(self, A, H):\n",
    "        Ms=torch.matmul(self.vs,H) # vs*H cf H=H(k)=W(k)H(k-1)\n",
    "        Ms=torch.matmul(A,Ms) # A*vs*H\n",
    "        Mr=torch.matmul(self.vr,H) # vr*H\n",
    "        Mr=torch.matmul(A,Mr) # A*vr*H\n",
    "        Mr=torch.transpose(Mr,(1,2))\n",
    "        C=torch.softmax(torch.sigmoid(Ms+Mr))\n",
    "\n",
    "        return C\n",
    "\n",
    "test=test(256, 128)\n",
    "print(test.W.shape)\n",
    "print(test.vr.shape)\n",
    "print(test.vs.shape)\n",
    "print(test.Graph_Attention_Layer(A, H=X))\n"
   ]
  },
  {
   "source": [
    "## model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#define GCNConv\n",
    "class GCNConv(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, **kwargs):\n",
    "        super(GCNConv, self).__init__(**kwargs)\n",
    "        self.W, self.vr, self.vs = self.define_weight(input_dim, output_dim)\n",
    "\n",
    "    def define_weight(self, input_dim, output_dim):\n",
    "        init_range = np.sqrt(6.0/(input_dim + output_dim))\n",
    "        W_init = torch.rand(input_dim, output_dim)*2*init_range - init_range\n",
    "        W = nn.Parameter(W_init)\n",
    "        v_init = torch.rand(1, output_dim)*2*init_range - init_range\n",
    "        vr = nn.Parameter(v_init)\n",
    "        vs = nn.Parameter(v_init)\n",
    "        return W, vr, vs\n",
    "\n",
    "    def Graph_Attention_Layer(self, A, H):\n",
    "        Ms=torch.matmul(self.vs,H) # vs*H cf H=H(k)=W(k)H(k-1)\n",
    "        Ms=torch.matmul(A,Ms) # A*vs*H\n",
    "        Mr=torch.matmul(self.vr,H) # vr*H\n",
    "        Mr=torch.matmul(A,Mr) # A*vr*H\n",
    "        Mr=torch.transpose(Mr,(1,2))\n",
    "        C=torch.softmax(torch.sigmoid(Ms+Mr))\n",
    "        return C\n",
    "\n",
    "    def forward(self, A, H):\n",
    "        # H = torch.matmul()\n",
    "        # self.C = self.Graph_Attention_Layer(H, A)\n",
    "        return 0\n",
    "\n",
    "#define model\n",
    "class gae(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(gae, self).__init__()\n",
    "        self.gcn = GCNConv(feature_dim, 128)\n",
    "        \n",
    "    def encode(self,):\n",
    "        return 0\n",
    "    def decode(self,):\n",
    "        return 0\n",
    "    def forward(self,):\n",
    "        X = encode(A, X)\n",
    "        X = decode(A, X)\n",
    "\n",
    "#define loss function\n",
    "def loss_fn():\n",
    "    loss=1\n",
    "    return loss\n"
   ]
  },
  {
   "source": [
    "## Training execution"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-dfb37ada84d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m99\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "# run model with epoches\n",
    "model = gae()\n",
    "\n",
    "loss_fn = lossfn()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\n",
    "\n",
    "epoches = cfg.total_epoches\n",
    "for t in range(epoches):\n",
    "    X, A, _, _ = train_dataloader\n",
    "\n",
    "    y_pred = model(X)\n",
    "    loss = loss_fn(y_pred, y_true)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'Result: {model.string()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}