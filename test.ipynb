{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.8 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# GATE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (create_logger, set_random_seed, rm_suffix,\n",
    "                   mkdir_if_no_exists)\n",
    "from utils import (read_meta, read_probs, l2norm, knns2ordered_nbrs,\n",
    "                   intdict2ndarray, Timer)\n",
    "from utils import (write_meta, write_feat)\n",
    "from utils.knn import *\n",
    "\n",
    "from mmcv import Config \n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n"
   ]
  },
  {
   "source": [
    "## load data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config='/dcs/pg20/u2085214/fc/learn-to-cluster/test_train_cfg_trial.py'\n",
    "cfg = Config.fromfile(config)\n",
    "\n",
    "cfg.phase = 'train'\n",
    "cfg.cuda = torch.cuda.is_available()\n",
    "cfg.load_from = None\n",
    "cfg.resume_from = None\n",
    "cfg.gpus = 1\n",
    "cfg.distributed = False\n",
    "cfg.save_output = False\n",
    "cfg.no_cuda = False\n",
    "cfg.force = False\n",
    "cfg.work_dir = './data/'\n",
    "cfg.cut_name = '_cut'\n",
    "\n",
    "for k, v in cfg.model['kwargs'].items(): #kwargs=dict(feature_dim=256)\n",
    "    setattr(cfg.train_data, k, v) #k? v?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from utils import (read_meta, read_probs, l2norm, knns2ordered_nbrs,\n",
    "                   intdict2ndarray, Timer)\n",
    "\n",
    "class ClusterDataset(object):\n",
    "    def __init__(self, cfg):\n",
    "        feat_path = cfg['feat_path']\n",
    "        label_path = cfg.get('label_path', None)\n",
    "        knn_graph_path = cfg['knn_graph_path']\n",
    "\n",
    "        self.k_at_hop = cfg['k_at_hop'] #k_at_hop=[100, 10], #200,10\n",
    "        self.depth = len(self.k_at_hop) #e.g. 2 : legth for k_at_hop\n",
    "        self.active_connection = cfg['active_connection']\n",
    "        self.feature_dim = cfg['feature_dim']\n",
    "        self.is_norm_feat = cfg.get('is_norm_feat', True) #normalized\n",
    "        self.is_sort_knns = cfg.get('is_sort_knns', True) #sorted knn\n",
    "        self.is_test = cfg.get('is_test', False) #depends on the train or test\n",
    "        \n",
    "        with Timer('read meta and feature'):\n",
    "            if label_path is not None:\n",
    "                _, idx2lb = read_meta(label_path) #e.g. {0:0,1:0,2:0 ...} dict format\n",
    "                self.inst_num = len(idx2lb) #instance num = # of data \n",
    "                self.labels = intdict2ndarray(idx2lb) #no. of class=8573 [   0.    0.    0. ... 8572. 8572. 8572.]\n",
    "                self.ignore_label = False\n",
    "            else:\n",
    "                self.labels = None\n",
    "                self.inst_num = -1\n",
    "                self.ignore_label = True\n",
    "            self.features = read_probs(feat_path, self.inst_num,\n",
    "                                       self.feature_dim)#self.feature.shape:(576494, 256)\n",
    "     \n",
    "            if self.is_norm_feat:\n",
    "                self.features = l2norm(self.features)\n",
    "            if self.inst_num == -1:\n",
    "                self.inst_num = self.features.shape[0]\n",
    "            self.size = self.inst_num\n",
    "\n",
    "        with Timer('read knn graph'):\n",
    "            knns = np.load(knn_graph_path)['data']\n",
    "            _, self.knn_graph = knns2ordered_nbrs(knns, sort=self.is_sort_knns)\n",
    "        assert np.mean(self.k_at_hop) >= self.active_connection\n",
    "\n",
    "        print('feature shape: {}, norm_feat: {}, sort_knns: {} '\n",
    "              'k_at_hop: {}, active_connection: {}'.format(\n",
    "                  self.features.shape, self.is_norm_feat, self.is_sort_knns,\n",
    "                  self.k_at_hop, self.active_connection))\n",
    "        print('labels shape:', self.labels.shape) #(584013,)\n",
    "        print('knns_graph shape:', self.knn_graph.shape) #(584013, 80)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        return the vertex feature and the adjacent matrix A, together\n",
    "        with the indices of the center node and its 1-hop nodes\n",
    "        '''\n",
    "        if index is None or index > self.size:\n",
    "            raise ValueError('index({}) is not in the range of {}'.format(\n",
    "                index, self.size))\n",
    "\n",
    "        center_node = index #428572 each index\n",
    "        \n",
    "        # hops[0] for 1-hop neighbors, hops[1] for 2-hop neighbors\n",
    "        hops = []\n",
    "        hops.append(set(self.knn_graph[center_node][1:]))\n",
    "        \n",
    "        # Actually we dont need the loop since the depth is fixed here,\n",
    "        # But we still remain the code for further revision\n",
    "        for d in range(1, self.depth):\n",
    "            hops.append(set())\n",
    "            for h in hops[-2]:\n",
    "                hops[-1].update(set(self.knn_graph[h][1:self.k_at_hop[d] + 1]))\n",
    "\n",
    "        hops_set = set([h for hop in hops for h in hop])\n",
    "        hops_set.update([\n",
    "            center_node,\n",
    "        ])\n",
    "\n",
    "        uniq_nodes = np.array(list(hops_set), dtype=np.int64)\n",
    "        uniq_nodes_map = {j: i for i, j in enumerate(uniq_nodes)}\n",
    "\n",
    "        center_idx = np.array([uniq_nodes_map[center_node]], dtype=np.int64)\n",
    "        one_hop_idxs = np.array([uniq_nodes_map[i] for i in hops[0]],\n",
    "                                dtype=np.int64)\n",
    "        center_feat = self.features[center_node]\n",
    "        feat = self.features[uniq_nodes]\n",
    "        feat = feat - center_feat\n",
    "\n",
    "        max_num_nodes = self.k_at_hop[0] * (self.k_at_hop[1] + 1) + 1\n",
    "        num_nodes = len(uniq_nodes)\n",
    "\n",
    "        # print('hops size[0]',len(hops[0]))\n",
    "        # print('hops size[1]',len(hops[1]))\n",
    "        # print('hops_set size',len(hops_set))\n",
    "        # print('index[{}] num_node: max={} uniq={}'.format(index,max_num_nodes,num_nodes))\n",
    "        # print('one_hop_idxs shape:',one_hop_idxs.shape)\n",
    "        # print('ceter_node:',center_node)\n",
    "        # print('feat shape',feat.shape)\n",
    "\n",
    "        A = np.zeros([num_nodes, num_nodes], dtype=feat.dtype)\n",
    "\n",
    "        res_num_nodes = max_num_nodes - num_nodes\n",
    "        if res_num_nodes > 0:\n",
    "            pad_feat = np.zeros([res_num_nodes, self.feature_dim],\n",
    "                                dtype=feat.dtype)\n",
    "            feat = np.concatenate([feat, pad_feat], axis=0)\n",
    "      \n",
    "        for node in uniq_nodes:\n",
    "            neighbors = self.knn_graph[node, 1:self.active_connection + 1]\n",
    "            for n in neighbors:\n",
    "                if n in uniq_nodes:\n",
    "                    i, j = uniq_nodes_map[node], uniq_nodes_map[n]\n",
    "                    A[i, j] = 1\n",
    "                    A[j, i] = 1\n",
    "\n",
    "        D = A.sum(1, keepdims=True)\n",
    "        A = A / D\n",
    "        A_ = np.zeros([max_num_nodes, max_num_nodes], dtype=A.dtype)\n",
    "        A_[:num_nodes, :num_nodes] = A\n",
    "\n",
    "        if self.ignore_label:\n",
    "            return (feat, A_, center_idx, one_hop_idxs)\n",
    "\n",
    "        labels = self.labels[uniq_nodes]\n",
    "        one_hop_labels = labels[one_hop_idxs]\n",
    "        center_label = labels[center_idx]\n",
    "        edge_labels = (center_label == one_hop_labels).astype(np.int64)\n",
    "\n",
    "        # print('feat shape={}, A_ shape={}, one_hop_idxs shape={},'.format(feat.shape))\n",
    "\n",
    "\n",
    "        if self.is_test:\n",
    "            if res_num_nodes > 0:\n",
    "                pad_nodes = np.zeros(res_num_nodes, dtype=uniq_nodes.dtype)\n",
    "                uniq_nodes = np.concatenate([uniq_nodes, pad_nodes], axis=0)\n",
    "            return (feat, A_, one_hop_idxs,\n",
    "                    edge_labels), center_idx, uniq_nodes\n",
    "        else:\n",
    "            return (feat, A_, one_hop_idxs, edge_labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmcv.runner import get_dist_info\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dsgcn.datasets.sampler import (DistributedSampler,\n",
    "                                    DistributedSequentialSampler)\n",
    "\n",
    "\n",
    "def build_dataloader(dataset,\n",
    "                     batch_size_per_gpu,\n",
    "                     workers_per_gpu,\n",
    "                     shuffle=False,\n",
    "                     train=False,\n",
    "                     **kwargs):\n",
    "    rank, world_size = get_dist_info()\n",
    "    if train:\n",
    "        sampler = DistributedSampler(dataset, world_size, rank, shuffle)\n",
    "    else:\n",
    "        sampler = DistributedSequentialSampler(dataset, world_size, rank)\n",
    "    batch_size = batch_size_per_gpu\n",
    "    num_workers = workers_per_gpu\n",
    "\n",
    "    data_loader = DataLoader(dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             sampler=sampler,\n",
    "                             num_workers=num_workers,\n",
    "                             pin_memory=False,\n",
    "                             **kwargs)\n",
    "\n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[./data/labels/part0_train_cut.meta] #cls: 45, #inst: 2948\n[Time] read meta and feature consumes 0.0139 s\n[Time] read knn graph consumes 0.0456 s\nfeature shape: (2948, 256), norm_feat: True, sort_knns: True k_at_hop: [50, 10], active_connection: 10\nlabels shape: (2948,)\nknns_graph shape: (2948, 80)\n"
     ]
    }
   ],
   "source": [
    "dataset = ClusterDataset(cfg.train_data)\n",
    "train_dataloader = build_dataloader(dataset,\n",
    "                         cfg.batch_size_per_gpu,\n",
    "                         cfg.workers_per_gpu,\n",
    "                         train=True,\n",
    "                         shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "one_hop_idxs shape=torch.Size([16, 79])\n one_hope_idxs e.g.=tensor([[  0,   1,   2,  ...,  91,  92,  94],\n        [  0,   1,   2,  ...,  87,  88,  90],\n        [  0,   1,   3,  ...,  88,  89,  90],\n        ...,\n        [  0,   1,   2,  ...,  82,  83,  84],\n        [ 14,  25,  28,  ..., 224, 225, 242],\n        [  0,   1,   2,  ...,  79,  80,  81]])\n\n\nA shape=torch.Size([16, 551, 551])\nA e.g.=tensor([[0.0000, 0.0476, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n        [0.0833, 0.0000, 0.0833,  ..., 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0455, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n        ...,\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])\n\n\nX shape=torch.Size([16, 551, 256])\nx e.g.=tensor([[ 0.0080,  0.0042, -0.0511,  ...,  0.0111,  0.0433,  0.0533],\n        [ 0.0620,  0.0388, -0.0499,  ...,  0.0383,  0.0207,  0.1123],\n        [ 0.0088,  0.0206, -0.0360,  ...,  0.0114,  0.0359,  0.0466],\n        ...,\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n\n\nlabels shape=torch.Size([16, 79])\nlabels e.g.=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1])\n\n\nlabels.view(-1) shape=torch.Size([1264])\nlabels.view(-1) e.g.=1\n\n\n"
     ]
    }
   ],
   "source": [
    "#print train_data.\n",
    "i=0\n",
    "for X, A, one_hop_idxs, labels in train_dataloader:\n",
    "    print('one_hop_idxs shape={}\\n one_hope_idxs e.g.={}\\n\\n'.format(one_hop_idxs.shape,one_hop_idxs))\n",
    "    print('A shape={}\\nA e.g.={}\\n\\n'.format(A.shape,A[0,:,:]))\n",
    "    print('X shape={}\\nx e.g.={}\\n\\n'.format(X.shape,X[0,:,:]))\n",
    "    # print('pred shape={}\\npred e.g.={}'.format(pred.shape,pred[0]))\n",
    "    print('labels shape={}\\nlabels e.g.={}\\n\\n'.format(labels.shape,labels[0]))\n",
    "    print('labels.view(-1) shape={}\\nlabels.view(-1) e.g.={}\\n\\n'.format(labels.view(-1).shape,labels.view(-1)[0]))\n",
    "    \n",
    "    i+=1\n",
    "    if(i==1):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mat1=torch.Size([5, 5]) mat2=torch.Size([2, 5, 5])\n@@mat1@@\n tensor([[1, 0, 0, 0, 1],\n        [0, 0, 1, 1, 0],\n        [1, 0, 1, 0, 0],\n        [1, 1, 1, 1, 1],\n        [0, 1, 1, 1, 0]])\nrow tensor([0, 0, 1, 1, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4])\ncol tensor([0, 4, 2, 3, 0, 2, 0, 1, 2, 3, 4, 1, 2, 3])\nresult :\n tensor([ 0,  0,  2,  3,  0,  4,  0,  3,  6,  9, 12,  4,  8, 12])\n"
     ]
    }
   ],
   "source": [
    "#simple matrix calculation and dimension check\n",
    "AA=torch.randint(2,size=(2,5,5))\n",
    "AA2=torch.randint(2,size=(2,5,5))\n",
    "AA3=torch.randint(2,size=(5,5))\n",
    "# XX=torch.rand(2,5,3)\n",
    "XX2=torch.randint(3,size=(2,5,3))\n",
    "XX3=torch.rand=(5,3)\n",
    "\n",
    "mat1=AA3\n",
    "mat2=AA2\n",
    "mat3=XX3\n",
    "mat4=XX2\n",
    "\n",
    "print('mat1={} mat2={}'.format(mat1.shape, mat2.shape))\n",
    "# print('mat3={} mat4={}'.format(mat3.shape, mat4.shape))\n",
    "print('@@mat1@@\\n', mat1)\n",
    "\n",
    "# res=torch.matmul(mat1,mat2)\n",
    "# res=torch.transpose(mat2,1,2)\n",
    "res1 = AA3\n",
    "res1 = (AA3 == 1).nonzero(as_tuple=True)\n",
    "row = res1[0]\n",
    "col = res1[1]\n",
    "print('row', row)\n",
    "print('col',col)\n",
    "print('result :\\n',row*col)\n",
    "# print('res1 shape:',res1.shape)\n",
    "\n",
    "# print('res2 result:', res2)\n",
    "# print('res2 shape:',res2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-b45a05d15be7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "torch.rand(5,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "256 128\n0.125\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-916cae27a25b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'W:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# print('vr:',test.vr.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-916cae27a25b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dim, output_dim, **kwargs)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mtest_class\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefine_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdefine_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-916cae27a25b>\u001b[0m in \u001b[0;36mdefine_weight\u001b[0;34m(self, input_dim, output_dim)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mW_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#*2*init_range - init_range\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "#test print shape\n",
    "class test_class:\n",
    "    def __init__(self, input_dim=0, output_dim=0, **kwargs):\n",
    "        self.W, self.vr, self.vs = self.define_weight(input_dim, output_dim)\n",
    "\n",
    "    def define_weight(self, input_dim, output_dim):\n",
    "        print(input_dim, output_dim)\n",
    "        init_range = np.sqrt(6.0/(input_dim + output_dim))\n",
    "        print(init_range)\n",
    "\n",
    "        W_init = torch.rand(input_dim, output_dim)#*2*init_range - init_range\n",
    "        print(W_init)\n",
    "        W = nn.Parameter(W_init)\n",
    "        v_init = torch.rand(output_dim,1)*2*init_range - init_range\n",
    "        vr = nn.Parameter(v_init)\n",
    "        vs = nn.Parameter(v_init)\n",
    "        return W, vr, vs\n",
    "\n",
    "    def Graph_Attention_Layer(self, A, X):\n",
    "        #compute layer k feature matrix\n",
    "        print('org X:',X.shape)\n",
    "        X=torch.matmul(X,self.W)\n",
    "        print('1st X:',X.shape)\n",
    "        \n",
    "        #compute Ms \n",
    "        Ms=torch.einsum('bnd,df->bnf', (X, self.vs))\n",
    "        Ms=torch.matmul(A,Ms) # A*vs*H\n",
    "        #compute Mr\n",
    "        Mr=torch.einsum('bnd,df->bnf', (X, self.vr)) # vr*H\n",
    "        Mr=torch.matmul(A,Mr) # A*vr*H\n",
    "        Mr=torch.transpose(Mr,1,2)\n",
    "\n",
    "        #compute attention \n",
    "        C=F.sigmoid(Ms+Mr)\n",
    "        C=F.softmax(C)\n",
    "        print('C:',C.shape)\n",
    "\n",
    "        #multiply X and attention score\n",
    "        X=torch.matmul(C, X)\n",
    "        print('new X:',X.shape)\n",
    "\n",
    "        return X\n",
    "\n",
    "test=test_class(256, 128)\n",
    "print('W:',test.W.shape)\n",
    "# print('vr:',test.vr.shape)\n",
    "# print('vs:',test.vs.shape)\n",
    "# print('A:',A.shape)\n",
    "# print('X:',X.shape)\n",
    "print(test_model.Graph_Attention_Layer(A, X))\n"
   ]
  },
  {
   "source": [
    "## model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-21-664988d1ed97>, line 72)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-664988d1ed97>\"\u001b[0;36m, line \u001b[0;32m72\u001b[0m\n\u001b[0;31m    graph_loss =\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#define GCNConv\n",
    "class GCNConv(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, **kwargs):\n",
    "        super(GCNConv,self).__init__(**kwargs)\n",
    "        self.W, self.vr, self.vs = self.define_weight(input_dim, output_dim)\n",
    "\n",
    "    def define_weight(self, input_dim, output_dim):\n",
    "        init_range = np.sqrt(6.0/(input_dim + output_dim))\n",
    "        W_init = torch.rand(input_dim, output_dim)*2*init_range - init_range\n",
    "        W = nn.Parameter(W_init)\n",
    "        v_init = torch.rand(output_dim,1)*2*init_range - init_range\n",
    "        vr = nn.Parameter(v_init)\n",
    "        vs = nn.Parameter(v_init)\n",
    "        return W, vr, vs\n",
    "\n",
    "    def Graph_Attention_Layer(self, A, X):\n",
    "        #compute layer k feature matrix\n",
    "        print('org X:', X.shape)\n",
    "        X=torch.matmul(X, self.W)\n",
    "        print('1st X:', X.shape)\n",
    "        \n",
    "        #compute Ms \n",
    "        Ms=torch.einsum('bnd,df->bnf', (X, self.vs))\n",
    "        Ms=torch.matmul(A,Ms) # A*vs*H\n",
    "        #compute Mr\n",
    "        Mr=torch.einsum('bnd,df->bnf', (X, self.vr)) # vr*H\n",
    "        Mr=torch.matmul(A,Mr) # A*vr*H\n",
    "        Mr=torch.transpose(Mr,1,2)\n",
    "\n",
    "        #compute attention \n",
    "        C=F.sigmoid(Ms+Mr)\n",
    "        C=F.softmax(C)\n",
    "        print('C:',C.shape)\n",
    "\n",
    "        #multiply X and attention score\n",
    "        X=torch.matmul(C, X)\n",
    "        print('new X:',X.shape)\n",
    "\n",
    "        return X\n",
    "\n",
    "#define model\n",
    "class gae(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(gae, self).__init__()\n",
    "        self.encode_gcn1 = GCNConv(feature_dim, 128)\n",
    "        self.encode_gcn2 = GCNConv(128, 128)\n",
    "        self.decode_gcn1 = GCNConv(128, 128)\n",
    "        self.decode_gcn2 = GCNConv(128, 256)\n",
    "        \n",
    "    def encoder(self, A, X):\n",
    "        X = self.encode_gcn1(A, X)\n",
    "        X = self.encode_gcn2(A, X)\n",
    "        return X\n",
    "\n",
    "    def decoder(self, A, X):\n",
    "        X = self.decode_gcn1(A, X)\n",
    "        X = self.decode_gcn2(A, X)\n",
    "        return X\n",
    "\n",
    "    def forward(self, data, return_loss=False):\n",
    "        X, A, one_hope_idxs, labels = data\n",
    "\n",
    "        X_org = X\n",
    "        X = encoder(A, X_org)\n",
    "\n",
    "        self.X_latent = X\n",
    "\n",
    "        X = decoder(A, X)\n",
    "        X_recon = X\n",
    "\n",
    "        feat_loss = torch.sqrt(torch.sum((X_org - X_recon)**2))\n",
    "        graph_loss = \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Training execution"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-dfb37ada84d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m99\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "# run model with epoches\n",
    "model = gae()\n",
    "# loss_fn = lossfn()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\n",
    "epoches = cfg.total_epoches\n",
    "\n",
    "for t in range(epoches):\n",
    "    X, A, _, _ = train_dataloader\n",
    "\n",
    "    y_pred, loss = model(X)\n",
    "    # loss = loss_fn(y_pred, y_true)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'Result: {model.string()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}